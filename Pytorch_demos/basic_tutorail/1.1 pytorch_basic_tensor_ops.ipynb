{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "介绍了pytorch中张量tensor的初始化、数据类型转换、数据共享机制和常见的数学操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1.post2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 序列化操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "torch.save(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载\n",
    "torch.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 两张量的比较操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [0, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 逐元素比较元素相等性torch.eq(input, other, out=None)\n",
    "torch.eq(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比较两个张量是否完全相等（形状和对应的元素值）torch.equal(tensor1, tensor2) → bool\n",
    "torch.equal(torch.Tensor([1, 2]), torch.Tensor([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.ge(input, other, out=None) → Tensor  逐元素比较input和other是否大于等于\n",
    "# torch.gt(input, other, out=None) → Tensor 逐元素比较input和other是否大于\n",
    "# torch.le(input, other, out=None) → Tensor  逐元素比较input和other是否小于等于\n",
    "# torch.lt(input, other, out=None) → Tensor 逐元素比较input和other是否小于"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8785)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取所有元素中的最大值\n",
    "a = torch.randn(1, 3)\n",
    "torch.max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.3239, 0.0625, 1.1158, 0.9539]), tensor([0, 2, 2, 3]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引。torch.max(input, dim, max=None, max_indices=None) -> (Tensor, LongTensor)\n",
    "a = torch.randn(4, 4)\n",
    "torch.max(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3595,  0.7085,  1.0766,  1.5260])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回两个tensor中对应元素的最大值\n",
    "a = torch.randn(4)\n",
    "b = torch.randn(4)\n",
    "torch.max(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 随机抽样操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 设定随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10a7aff30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设定生成随机数的种子，并返回一个 torch._C.Generator 对象.\n",
    "torch.manual_seed(66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回生成随机数的原始种子值\n",
    "torch.initial_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([66,  0,  0,  ...,  0,  0,  0], dtype=torch.uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回随机生成器状态\n",
    "torch.get_rng_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 数据分布生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 伯努利分布torch.bernoulli(input, out=None) → Tensor\n",
    "# input中各元素为伯努利分布的概率分布（各元素值必须在0-1），返回一个同shape的0-1张量\n",
    "torch.bernoulli(torch.rand(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 1, 1],\n",
       "        [0, 2, 0, 3]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多项式分布torch.multinomial(input, num_samples,replacement=False, out=None) → LongTensor\n",
    "# input中各行表示多项式分部概率（不能全部为0，会自动进行weight的归一化）\n",
    "# replacement=True表示有放回采样\n",
    "weights = torch.Tensor([[0, 10, 3, 0], [1,2,3,4]])\n",
    "torch.multinomial(weights, 4, replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.1075,  0.1147, -0.1127,  0.7183,  0.9590,  2.1193,  0.6652,\n",
       "         1.4297,  1.6131])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正态分布生成\n",
    "# mean和std为对应的两个张量，每个（i，j）表示一个独立的分布进行采用\n",
    "bb=torch.Tensor(2,2)\n",
    "torch.normal(mean=torch.linspace(0,1,10), std=torch.linspace(0,1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7449, -1.7071, -4.6307,  4.2131,  3.6009,  4.9264, -3.2759, -1.5770,\n",
       "        -2.8447,  8.8612])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同均值，不同方差\n",
    "torch.normal(mean=0.5, std=torch.linspace(1, 6,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7144, 2.1387, 1.8347, 2.5222, 2.9733, 4.8278, 4.4823, 5.1017, 6.0186,\n",
       "        6.0603])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同方差，不同均值\n",
    "torch.normal(std=0.5, mean=torch.linspace(1, 6,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 张量的索引、切片、连接操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1693,  0.4543, -1.4110],\n",
       "        [ 0.3194,  0.6029,  0.4305]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1693,  0.4543, -1.4110],\n",
      "        [ 0.3194,  0.6029,  0.4305],\n",
      "        [ 1.1693,  0.4543, -1.4110],\n",
      "        [ 0.3194,  0.6029,  0.4305]])\n",
      "tensor([[ 1.1693,  0.4543, -1.4110,  1.1693,  0.4543, -1.4110],\n",
      "        [ 0.3194,  0.6029,  0.4305,  0.3194,  0.6029,  0.4305]])\n"
     ]
    }
   ],
   "source": [
    "# 沿着某个已有的axis进行拼接  torch.cat(inputs, dimension=0) → Tensor\n",
    "print(torch.cat((x,x), dim=0))   # dim=0即行方向\n",
    "print(torch.cat((x,x), dim=1))   # dim=0即行方向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.9716,  0.0812,  0.3391],\n",
       "         [-0.7621,  1.3135, -2.3922]]), tensor([[-0.9716,  0.0812,  0.3391],\n",
       "         [-0.7621,  1.3135, -2.3922]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 沿着某个已有的axis进行分块（cat的反向操作） torch.chunk(tensor, chunks, dim=0)\n",
    "# chunks为个数\n",
    "x = torch.randn(2,3)\n",
    "y = torch.cat((x,x), dim=0)   # 先拼接\n",
    "torch.chunk(y, 2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5554, -0.0718,  0.3904],\n",
      "        [-1.7151, -0.1270,  1.3241],\n",
      "        [ 0.7957,  1.1578,  0.3443]])\n",
      "(tensor([[ 1.5554, -0.0718,  0.3904],\n",
      "        [-1.7151, -0.1270,  1.3241]]), tensor([[0.7957, 1.1578, 0.3443]]))\n"
     ]
    }
   ],
   "source": [
    "# 沿着某个已有的axis进行分块（cat的反向操作） torch.chunk(tensor, split_size, dim=0)\n",
    "# split_size为每块的个数，如果沿指定维的张量形状大小不能被整分，则最后一个分块会小于其它分块\n",
    "x = torch.randn(3,3)\n",
    "print(x)\n",
    "print(torch.split(x, 2, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0326,  1.6117,  1.4897,  0.7246],\n",
       "        [-1.0571, -0.7701, -0.0142,  0.2322]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 沿着指定维度dim对输入进行index切片, torch.index_select(input, dim, index, out=None) → Tensor\n",
    "# index为一个LongTensor类型\n",
    "# 返回的张量不与原始张量共享内存空间\n",
    "indices=torch.LongTensor([0,2]) \n",
    "torch.index_select(x,0,index=indices)   # 验证dim=0的方向取其中的0、2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0326,  1.6117,  1.4897,  0.7246],\n",
      "        [-1.0571, -0.7701, -0.0142,  0.2322]])\n",
      "tensor([[-0.0326,  1.6117,  1.4897,  0.7246],\n",
      "        [ 0.0246,  1.1675,  1.0860, -0.9570],\n",
      "        [-1.0571, -0.7701, -0.0142,  0.2322]])\n"
     ]
    }
   ],
   "source": [
    "z = x[[0,2]]   # 同这种直接切片，同样会生成一个新的\n",
    "print(z)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0578, 1.1777, 0.1858],\n",
      "        [0.9285, 0.2916, 0.7150]])\n",
      "tensor([[1, 1, 1],\n",
      "        [0, 0, 1]], dtype=torch.uint8)\n",
      "tensor([1.0578, 1.1777, 0.1858, 0.7150])\n"
     ]
    }
   ],
   "source": [
    "# 掩码选择torch.masked_select(input, mask, out=None) → Tensor\n",
    "# input和mask的shape必须一样,mask必须为type类型张量\n",
    "# 返回值为一个一维向量，元素为对应mask元素为0的\n",
    "x = torch.randn(2, 3)\n",
    "mask = torch.bernoulli(torch.rand(2,3)).byte()\n",
    "y = torch.masked_select(x, mask)\n",
    "print(x)\n",
    "print(mask)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [1, 1],\n",
       "        [2, 2],\n",
       "        [3, 3]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.nonzero(input, out=None) → LongTensor返回非零元素索引的张量\n",
    "# 如果输入input有n维，则输出的索引张量output的形状为 z x n, 这里 z 是输入张量input中所有非零元素的个数。\n",
    "torch.nonzero(torch.Tensor([[0.6, 0.0, 0.0, 0.0],[0.0, 0.4, 0.0, 0.0],[0.0, 0.0, 1.2, 0.0],[0.0, 0.0, 0.0,-0.4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2, 1, 2])\n",
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 2, 1, 2])\n",
      "torch.Size([2, 1, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# torch.squeeze(input, dim=None, out=None)沿着指定轴进行挤压（若指定轴为1）\n",
    "# 若不声明dim，则会在所有值为1的dim上进行挤压\n",
    "# squeeze操作会共享内存\n",
    "x = torch.zeros(2,1,2,1,2)\n",
    "print(x.size())\n",
    "y = torch.squeeze(x)\n",
    "print(y.size())\n",
    "z = torch.squeeze(x,1)\n",
    "print(z.size())\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.unsqueeze(input, dim, out=None) 返回一个新的张量，对输入的制定位置插入维度 1\n",
    "# 返回张量与输入张量共享内存\n",
    "# 如果dim为负，则将会被转化dim+input.dim()+1\n",
    "x = torch.Tensor([1, 2, 3, 4])\n",
    "torch.unsqueeze(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1753,  1.3432, -1.6604],\n",
      "        [-1.4470,  1.2004,  0.1172]])\n",
      "tensor([[-0.1753, -1.4470],\n",
      "        [ 1.3432,  1.2004],\n",
      "        [-1.6604,  0.1172]])\n"
     ]
    }
   ],
   "source": [
    "# torch.t(input, out=None) → Tensor  二维张量的转置，等同于transpose(input, 0, 1)\n",
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "print(torch.transpose(x,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6668,  1.2261,  0.4297],\n",
      "        [-0.2251, -1.6966,  0.0579]])\n",
      "tensor([[-1.6668, -0.2251],\n",
      "        [ 1.2261, -1.6966],\n",
      "        [ 0.4297,  0.0579]])\n"
     ]
    }
   ],
   "source": [
    "# torch.transpose(input, dim0, dim1, out=None) → Tensor 交换dim0和dim1两个轴\n",
    "# 数据共享\n",
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "print(torch.transpose(x,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. CPU和GPU间的移动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor默认存储在CPU中，可以使用cuda方法移动到GPU中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_cpu = torch.zeros(3,4)\n",
    "tensor_cpu.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 并不共享内存，本计算机没GPU，GPU上跑下来的结果为'torch.cuda.FloatTensor'\n",
    "tensor_gpu = tensor_cpu.cuda()\n",
    "tensor_gpu.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#使用torch.cuda.is_available()来确定是否有cuda设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将tensor传送到设备\n",
    "gpu_x=cpu_x.to(device)\n",
    "gpu_x.type()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
